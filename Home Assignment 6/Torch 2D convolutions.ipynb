{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "structured-tuning",
   "metadata": {},
   "source": [
    "# PyTorch 2D convolutions\n",
    "#### Christian Igel, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-craft",
   "metadata": {},
   "source": [
    "## One input channel, one output, no padding\n",
    "Let's define a `W`$\\times$`W` filter. For the following examples, we do not need a bias parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution filter is of size W\n",
    "W = 3\n",
    "# 1 input (image) channel, 1 output channel, WxW convolution kernel\n",
    "conv = nn.Conv2d(1, 1, W, bias=False)\n",
    "print(\"We just defined:\", conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-throat",
   "metadata": {},
   "source": [
    "Let's look at the kernel dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-discovery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 output channel, 1 input channel, 1st dimension = W, 2nd dimension = W\n",
    "print(conv.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-working",
   "metadata": {},
   "source": [
    "The filter parameters are initialized randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-monaco",
   "metadata": {},
   "source": [
    "We can set the parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "print(conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-evolution",
   "metadata": {},
   "source": [
    "Let's define an input (image) `x`. The input is of the same shape as the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-spank",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(float(W*W))\n",
    "x = torch.reshape(x, (1, 1, W, W))\n",
    "print('Input:\\n', x)\n",
    "print('Sum of all input elements:', torch.sum(x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-benjamin",
   "metadata": {},
   "source": [
    "Because there is no padding and input and filter have the same size, there is only one valid position for the filter. Accordingly, the result is a tensor with a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conv(x)\n",
    "print('Tensor:', c, 'scalar:', c.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-positive",
   "metadata": {},
   "source": [
    "The scalar should be equal to the sum of all input elements (ensure that you understand why)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-stomach",
   "metadata": {},
   "source": [
    "## One input channel, one output,  padding\n",
    "Now we add zero-padding such that the input dimensionality is preseved:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 1, W, padding=W//2, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "c = conv(x)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-communication",
   "metadata": {},
   "source": [
    "## Several input channels, one output, no padding\n",
    "Typically, the input to a convolutional layer consists of several feature maps or channels. For example, consider a 2D input with three channels (e.g., an RGB colour image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(float(3*W*W))\n",
    "x = torch.reshape(x, (1, 3, W, W))\n",
    "print('Input:', x)\n",
    "print('Sum of all inputs:', torch.sum(x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-porter",
   "metadata": {},
   "source": [
    "Let's define a convolutional layer that takes three channels as input and produces a single output feature map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 input (image) channels, 1 output channel, WxW convolution kernel\n",
    "conv = nn.Conv2d(3, 1, W, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "print('Weight parameters of convolutional layer:', conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-wrapping",
   "metadata": {},
   "source": [
    "Note that there is one filter for each input channel.\n",
    "The convolutional layer first convolves each input channel with the corresponding filter.\n",
    "This results in three feature maps, whih are added to give the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-prayer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = conv(x)\n",
    "print('number of filter parameters:', conv.weight.numel(), '\\nresult of filtering the input:', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-calcium",
   "metadata": {},
   "source": [
    "It is important that the number of parameters and the dimesionality of the result is clear to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-antarctica",
   "metadata": {},
   "source": [
    "Now let's apply 1$\\times$1 convolutions to our three input channels. Again, we set all filter weights to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-front",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 input (image) channels, 1 output channel, 1x1 convolution kernel\n",
    "conv = nn.Conv2d(3, 1, 1, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "print(conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-resort",
   "metadata": {},
   "source": [
    "This convolutional layer adds the three input feature maps/channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-sharing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = conv(x)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-liberty",
   "metadata": {},
   "source": [
    "Thus, 1$\\times$1 convolutions can be used to compute weighted sums of input feature maps/channels (in our previous example, all weights were set to 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-rolling",
   "metadata": {},
   "source": [
    "## Several output maps\n",
    "Typically, convolutional layer produce several feature maps or channels. For example, consider \n",
    "extending the previous 1$\\times$1 example to two output maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-reform",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 input (image) channels, 2 output channel, 1x1 convolution kernel\n",
    "conv = nn.Conv2d(3, 2, 1, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "print(conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-while",
   "metadata": {},
   "source": [
    "This layer maps 3 input feature maps to 2 output feature maps, which are identical in our example, because we initialized all filters so that they are identical: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-dallas",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = conv(x)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-arrow",
   "metadata": {},
   "source": [
    "The first convolutional layer in a network has typically more output feature maps than input channels. Let's assume 3 input channels, 4 output channels of the same dimensionality (i.e., we use padding), and a filter size of 3. For each output channel, we have 3 filter with 9 parameters/weights each. Thus, we have 108 parameters in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-renewal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 4, W, padding=W//2, bias=False)\n",
    "print(conv.weight)\n",
    "print(\"Number of parameters:\", conv.weight.shape.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-chair",
   "metadata": {},
   "source": [
    "And here are the resulting feature maps when applied to our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conv(x)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-ozone",
   "metadata": {},
   "source": [
    "# Image processing examples\n",
    "Now we consider a more complex example that involves some basic image transformations. First, we need to import the torch image utilities and matplot for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-demographic",
   "metadata": {},
   "source": [
    "Let's generate an image and save it in JPEG format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70943ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"ML\"\n",
    "ax = plt.subplot()\n",
    "ax.axis('off')\n",
    "plt.text(0.5, 0.5, text, size=200,\n",
    "         ha=\"center\", va=\"center\")\n",
    "plt.savefig(text+\".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263e8db",
   "metadata": {},
   "source": [
    "Now we load the image again and convert it to grayscale so that we just deal with a single channel. The object returned by `read_image` is a tensor. We cast this tensor to a float tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe3a46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = read_image(text+\".jpg\", mode=ImageReadMode.GRAY).type(torch.FloatTensor)\n",
    "print(\"Tensor shape:\", image.shape, \"type:\", image.type(), \"min:\", image.min().item(), \"max:\", image.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-netscape",
   "metadata": {},
   "source": [
    "Let's plot the image. The tensor has the channels (here only one) as the first dimension. For an image, this is typically the last dimension, so we swap the dimensions for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-coordinate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1, 2, 0), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-drill",
   "metadata": {},
   "source": [
    "In order to be process by a layer, the tensor needs  another dimension/axis for enumerating the elements in a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.unsqueeze_(0)  # Add a dimension\n",
    "print(\"Shape after adding batch dimension:\", x.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-rainbow",
   "metadata": {},
   "source": [
    "Now we apply a simple horizontal gradient filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = torch.tensor([[[[-1., 1.]]]])  # Define filter\n",
    "print(\"Kernel:\", hf, \"shape:\", hf.shape)\n",
    "\n",
    "conv = nn.Conv2d(1, 1, kernel_size=(1, 2), padding=(0, 1), bias=False)  # Padding only in one dimension needed\n",
    "conv.weight = torch.nn.Parameter(hf, requires_grad=False)  # Set kernel parameters to predefined filter parameters  \n",
    "c = conv(x)  # Apply filter\n",
    "print(\"Tensor shape:\", c.shape, \"min:\", c.min().item(), \"max:\", c.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-comedy",
   "metadata": {},
   "source": [
    "We do not need a gradient for the kernel parameters, so we can use ``requires_grad=False``. This allows us to use ``c[0.0]`` as a NumPy array in the visualizaiton below. Alternatively, we could use ``c[0,0].detach()`` in the ``imshow`` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-trail",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(c[0,0].shape)\n",
    "plt.imshow(c[0,0], cmap='gray');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
